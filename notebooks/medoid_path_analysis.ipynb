{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bcccb4",
   "metadata": {},
   "source": [
    "# Medoid Path & Probability Bands Analysis\n",
    "\n",
    "This notebook demonstrates the **Medoid** approach for identifying the \"most likely path\" within a collection\n",
    "of time series.\n",
    "\n",
    "We will:\n",
    "1. Download **Ethereum (ETH-USD)** 1-hour data.\n",
    "2. Transform the continuous stream into a collection of **daily paths** (24 hours per day).\n",
    "3. Determine the **Medoid Path** among these daily paths.\n",
    "4. Calculate **Probability Bands** (Statistical Channel) around the Medoid.\n",
    "5. Visualize the results using **Plotly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b27820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go  # type: ignore\n",
    "from plotly.subplots import make_subplots  # type: ignore\n",
    "\n",
    "# Add the src directory to the system path to allow importing modules\n",
    "# pylint: disable=wrong-import-position\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Import the data loader\n",
    "from src.dataio.yfinance_loader import fetch_market_data  # pylint: disable=wrong-import-position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabcec9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 1. Core Algorithm: Medoid & Probability Channel\n",
    "\n",
    "We define the function `get_most_likely_channel` which:\n",
    "- Calculates the Euclidean distance matrix between all paths.\n",
    "- Identifies the Medoid (the path with the minimum sum of squared distances to all other paths).\n",
    "- Calculates percentiles for the probability bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_likely_channel(input_matrix: np.ndarray, probability: float = 0.75) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Finds the Medoid path and calculating the probability channel.\n",
    "\n",
    "    Args:\n",
    "        input_matrix (np.ndarray): Shape (n_series, time_steps)\n",
    "        probability (float): The coverage probability for the bands (e.g., 0.75 for 75%).\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains medoid path, lower bound, upper bound, and the index of the medoid.\n",
    "    \"\"\"\n",
    "    # Step 1: Find the Medoid (The Most Likely single path)\n",
    "    # input_matrix shape: (n_series, time_steps)\n",
    "    # We use broadcasting to compute differences: (N, 1, T) - (1, N, T) -> (N, N, T)\n",
    "    diffs = input_matrix[:, np.newaxis, :] - input_matrix[np.newaxis, :, :]\n",
    "\n",
    "    # Euclidean distance between each pair of paths\n",
    "    dist_matrix = np.sqrt(np.sum(np.square(diffs), axis=2))\n",
    "\n",
    "    # The Medoid minimizes the sum of distances to all other points\n",
    "    medoid_idx = np.argmin(np.sum(dist_matrix, axis=1))\n",
    "    most_likely_path = input_matrix[medoid_idx]\n",
    "\n",
    "    # Step 2: Calculate the Statistical Channel (Probability Range)\n",
    "    # We calculate the distribution at each time step t across all N series\n",
    "    tail = (1.0 - probability) / 2.0\n",
    "    lower_bound = np.percentile(input_matrix, tail * 100, axis=0)\n",
    "    upper_bound = np.percentile(input_matrix, (1.0 - tail) * 100, axis=0)\n",
    "\n",
    "    return {\"medoid\": most_likely_path, \"lower\": lower_bound, \"upper\": upper_bound, \"index\": medoid_idx}\n",
    "\n",
    "\n",
    "def find_nearest_neighbors(\n",
    "    partial_path: np.ndarray, history_matrix: np.ndarray, n_neighbors: int = 1\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the nearest neighbors in the history matrix for a given partial path.\n",
    "\n",
    "    Args:\n",
    "        partial_path (np.ndarray): Shape (k,) - The first k hours of a day.\n",
    "        history_matrix (np.ndarray): Shape (N, 24) - The historical dataset.\n",
    "        n_neighbors (int): Number of top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of matches, each containing matched path, index, distance, and rank.\n",
    "    \"\"\"\n",
    "    k = len(partial_path)\n",
    "    if k == 0:\n",
    "        raise ValueError(\"Partial path cannot be empty.\")\n",
    "\n",
    "    # Slice history to the same length as partial_path\n",
    "    history_slice = history_matrix[:, :k]\n",
    "\n",
    "    # Calculate Euclidean distance between partial_path and all historical segments\n",
    "    # Shape: (N,)\n",
    "    distances = np.sqrt(np.sum(np.square(history_slice - partial_path), axis=1))\n",
    "\n",
    "    # Find the indices of the k smallest distances\n",
    "    # np.argsort returns indices that sort the array. We take the first n_neighbors.\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    top_indices = sorted_indices[:n_neighbors]\n",
    "\n",
    "    matches = []\n",
    "    for rank, neighbor_idx in enumerate(top_indices):\n",
    "        matches.append(\n",
    "            {\n",
    "                \"matched_path\": history_matrix[neighbor_idx],\n",
    "                \"index\": neighbor_idx,\n",
    "                \"distance\": distances[neighbor_idx],\n",
    "                \"rank\": rank + 1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccfb1e",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition & Processing\n",
    "\n",
    "We fetch 1-hour data for ETH-USD. We aim for ~2 years of data to get a robust sample of daily behaviors.\n",
    "\n",
    "**Transformation Logic:**\n",
    "- We split the data into 24-hour chunks (Days).\n",
    "- For each day, we calculate the cumulative Log-Returns starting from a common reference point `P0`.\n",
    "- `P0` is defined as the *Close price of the previous day* (hour 23 of day D-1).\n",
    "- The path for Day D at hour h (0-23) is: $ \\ln(P_{D,h} / P_{D-1, 23}) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch Data\n",
    "SYMBOL = \"ETH-USD\"\n",
    "PERIOD = \"730d\"  # ~2 Years\n",
    "INTERVAL = \"1h\"\n",
    "\n",
    "print(f\"Fetching {SYMBOL} data for the last {PERIOD}...\")\n",
    "df = fetch_market_data(SYMBOL, PERIOD, INTERVAL)\n",
    "\n",
    "if df is None or df.empty:\n",
    "    raise ValueError(f\"No data fetched for {SYMBOL}\")\n",
    "\n",
    "# The loader returns a DataFrame with a 'Date' or 'Datetime' column (reset index).\n",
    "# We need to set it back to the index for our time-based processing.\n",
    "# Typically for 1h data, 'Datetime' is the column name.\n",
    "DATE_COLUMN = \"Datetime\" if \"Datetime\" in df.columns else \"Date\"\n",
    "if DATE_COLUMN in df.columns:\n",
    "    df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN])\n",
    "    df.set_index(DATE_COLUMN, inplace=True)\n",
    "\n",
    "# Ensure simple index if MultiIndex columns are present (just in case)\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Keep only Close price\n",
    "df = df[[\"Close\"]].copy()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de986184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Reshape into Daily Paths\n",
    "\n",
    "# Add date and hour info\n",
    "# Cast index to DatetimeIndex to ensure .date and .hour properties are accessible for MyPy\n",
    "if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df[\"Date\"] = df.index.date\n",
    "df[\"Hour\"] = df.index.hour\n",
    "\n",
    "# Pivot to create a matrix where rows are Dates and columns are Hours (0..23)\n",
    "# This might have missing values if some hours are missing\n",
    "daily_matrix_df = df.pivot(index=\"Date\", columns=\"Hour\", values=\"Close\")\n",
    "\n",
    "# 2a. Separate \"Live\" (Incomplete) Day from History\n",
    "# Check if the last row is incomplete (has NaNs)\n",
    "last_date = daily_matrix_df.index[-1]\n",
    "last_row = daily_matrix_df.iloc[-1]\n",
    "\n",
    "LIVE_DAY_SERIES = None\n",
    "LIVE_DAY_DATE = None\n",
    "\n",
    "if last_row.isna().any():\n",
    "    print(f\"Detected incomplete (live) data for: {last_date}\")\n",
    "    # Extract valid values for the live day\n",
    "    live_day_prices = last_row.dropna().values\n",
    "    if len(live_day_prices) > 0:\n",
    "        LIVE_DAY_DATE = last_date\n",
    "        # We need the closest previous close (P0) to calculate returns\n",
    "        # The previous row in the pivot table (even if not strictly yesterday, it's the last known data)\n",
    "        # Note: Ideally we want strictly the previous day.\n",
    "        # If the dataframe is sorted, row -2 is the previous day relative to row -1.\n",
    "        prev_day_row = daily_matrix_df.iloc[-2]\n",
    "\n",
    "        # P0 is the last value (hour 23) of the previous row.\n",
    "        # If previous row is also incomplete at hour 23, we can't reliably calc P0 for standard logic.\n",
    "        # We assume history is mostly complete.\n",
    "        if not np.isnan(prev_day_row[23]):\n",
    "            p0_live = prev_day_row[23]\n",
    "            LIVE_DAY_SERIES = np.log(live_day_prices / p0_live)\n",
    "            print(f\"Live day processed. Current hours: {len(LIVE_DAY_SERIES)}\")\n",
    "        else:\n",
    "            print(\"Cannot calculate live returns: Previous day hour 23 is missing.\")\n",
    "\n",
    "    # Remove the incomplete day from the dataframe so it doesn't mess up 'complete days' logic\n",
    "    daily_matrix_df = daily_matrix_df.drop(last_date)\n",
    "\n",
    "# Filter for complete days (must have exactly 24 hours)\n",
    "daily_matrix_df = daily_matrix_df.dropna()\n",
    "print(f\"Data reshaped. Complete days found: {len(daily_matrix_df)}\")\n",
    "\n",
    "# 3. Calculate Log-Returns from Previous Close (P0)\n",
    "\n",
    "# We need the previous day's close for each day in our filtered list.\n",
    "# Since we might have gaps after filtering, we look back at the original df or shift the pivoted one\n",
    "# if days are consecutive. A robust way is to re-index the pivoted DF to ensure we can access row i-1.\n",
    "\n",
    "dates = daily_matrix_df.index\n",
    "valid_paths = []\n",
    "valid_dates = []\n",
    "\n",
    "price_matrix = daily_matrix_df.values  # Shape (N, 24)\n",
    "\n",
    "# We iterate from index 1 because we need index 0 as the 'previous day' for P0 reference\n",
    "for i in range(1, len(price_matrix)):\n",
    "    current_day_prices = price_matrix[i]\n",
    "    prev_day_prices = price_matrix[i - 1]\n",
    "\n",
    "    # Check if dates are consecutive (optional, but good for \"Previous Close\" logic)\n",
    "    # If dates are not consecutive, using the previous row's close might be inaccurate if the gap is large.\n",
    "    # However, for general \"gap\" analysis, comparing to the last known close is often acceptable.\n",
    "    # Let's verify temporal continuity.\n",
    "    date_curr = dates[i]\n",
    "    date_prev = dates[i - 1]\n",
    "\n",
    "    if (date_curr - date_prev) <= timedelta(days=1):\n",
    "        # P0 is the last hour (index 23) of the previous row\n",
    "        p0 = prev_day_prices[-1]\n",
    "\n",
    "        # Log Returns relative to P0\n",
    "        # path[t] = ln( P_t / p0 )\n",
    "        log_returns_path = np.log(current_day_prices / p0)\n",
    "\n",
    "        valid_paths.append(log_returns_path)\n",
    "        valid_dates.append(date_curr)\n",
    "\n",
    "series_matrix = np.array(valid_paths)\n",
    "print(f\"Final Series Matrix shape: {series_matrix.shape} (Days x Hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676586d",
   "metadata": {},
   "source": [
    "## 3. Analysis\n",
    "We run the Medoid algorithm on our collection of log-return paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBABILITY = 0.75\n",
    "result = get_most_likely_channel(series_matrix, probability=PROBABILITY)\n",
    "\n",
    "medoid_path = result[\"medoid\"]\n",
    "lower_path = result[\"lower\"]\n",
    "upper_path = result[\"upper\"]\n",
    "medoid_date = valid_dates[result[\"index\"]]\n",
    "\n",
    "print(f\"Medoid Index: {result['index']}\")\n",
    "print(f\"Most Representative Date: {medoid_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa63d42",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "We use Plotly to create an interactive chart showing:\n",
    "1. **All Paths**: Faint, to show density.\n",
    "2. **The Medoid**: Highlighted, representing the \"center\" of behavior.\n",
    "3. **Probability Bands**: The statistical \"normal\" range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = np.arange(24)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# 1. Plot all background paths (Sampling a subset if too many for performance, e.g., max 500)\n",
    "MAX_PATHS_TO_PLOT = 500\n",
    "indices = np.arange(len(series_matrix))\n",
    "if len(indices) > MAX_PATHS_TO_PLOT:\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:MAX_PATHS_TO_PLOT]\n",
    "\n",
    "for idx in indices:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=series_matrix[idx],\n",
    "            mode=\"lines\",\n",
    "            line={\"color\": \"rgba(150, 150, 150, 0.1)\", \"width\": 1},\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"skip\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 2. Plot Probability Bands (Filled Area)\n",
    "# Upper Bound\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_steps, y=upper_path, mode=\"lines\", line={\"width\": 0}, showlegend=False, name=\"Upper Limit\")\n",
    ")\n",
    "\n",
    "# Lower Bound (with fill)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=lower_path,\n",
    "        mode=\"lines\",\n",
    "        line={\"width\": 0},\n",
    "        fill=\"tonexty\",  # Fill to the trace before it (Upper Limit)\n",
    "        fillcolor=\"rgba(0, 200, 255, 0.2)\",\n",
    "        showlegend=True,\n",
    "        name=f\"{int(PROBABILITY*100)}% Probability Band\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Plot Medoid Path\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_steps, y=medoid_path, mode=\"lines\", line={\"color\": \"blue\", \"width\": 4}, name=\"Medoid (Most Likely Path)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Medoid Path Analysis: ETH-USD Daily 1H Profiles (Sample of {len(series_matrix)} days)\",\n",
    "    xaxis_title=\"Hour of Day (0-23 UTC)\",\n",
    "    yaxis_title=\"Log Return (vs Prev Day Close)\",\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"x unified\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44915d72",
   "metadata": {},
   "source": [
    "## 5. Partial Path Matching (Forecasting / Nearest Neighbor)\n",
    "\n",
    "Here we simulate a real-world scenario:\n",
    "- We have observed the starting $k$ hours of a new day.\n",
    "- We want to find the **most similar day** from history to predict how the rest of the day might unfold.\n",
    "\n",
    "**Procedure:**\n",
    "1. Select a \"Target Day\" (can be the latest day or a specific historic date).\n",
    "2. Remove this Target Day from the \"Historical Library\" (to avoid matching with itself).\n",
    "3. For each $k$ in user-defined `HOURS_TO_TEST` (e.g. [6, 12, 18, 23]):\n",
    "    - Take the first $k$ hours of the Target Day.\n",
    "    - Find the nearest neighbor in the Library (Euclidean distance on first $k$ steps).\n",
    "    - Plot the Input vs. the Matched Outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b50111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ---------------------------------------------------------\n",
    "# Define the hours you want to check for partial matching\n",
    "# NOTE: If we are in \"Live\" mode (TARGET_DATE=None and live data exists),\n",
    "# this list is ignored, and we only test the current available hours.\n",
    "HOURS_TO_TEST = [6, 12, 18, 23]\n",
    "\n",
    "# Number of nearest neighbors (top matches) to find and display\n",
    "NUM_MATCHES = 3\n",
    "\n",
    "# Define a specific target date to analyze (YYYY-MM-DD or None).\n",
    "# If None, the script will use:\n",
    "#   1. The LIVE incomplete day (if available) -> This is the \"Real-Time Forecast\".\n",
    "#   2. The LAST complete historical day (if no live data is available).\n",
    "TARGET_DATE = None  # e.g., \"2024-01-15\"\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Use the full history for matching library, unless we select a target from it\n",
    "library_matrix = series_matrix\n",
    "library_dates = valid_dates\n",
    "\n",
    "# Variables to hold our target info\n",
    "TARGET_PATH_FULL = None  # May be None if live\n",
    "TARGET_PATH_PARTIAL = None\n",
    "TARGET_DATE_STR = \"\"\n",
    "IS_LIVE_MODE = False\n",
    "\n",
    "# 1. Determine Target Logic\n",
    "if TARGET_DATE:\n",
    "    # --- A. Specific Historic Date ---\n",
    "    print(f\"Mode: Historic Specific Date ({TARGET_DATE})\")\n",
    "    try:\n",
    "        t_date_obj = pd.to_datetime(TARGET_DATE).date()\n",
    "        date_indices = [i for i, d in enumerate(valid_dates) if d == t_date_obj]\n",
    "\n",
    "        if not date_indices:\n",
    "            raise ValueError(f\"Date {TARGET_DATE} not found in valid history.\")\n",
    "\n",
    "        TARGET_IDX = date_indices[0]\n",
    "\n",
    "        # Remove target from library so we don't match with self\n",
    "        library_indices = np.arange(len(series_matrix)) != TARGET_IDX\n",
    "        library_matrix = series_matrix[library_indices]\n",
    "        library_dates = [d for i, d in enumerate(valid_dates) if i != TARGET_IDX]\n",
    "\n",
    "        TARGET_PATH_FULL = series_matrix[TARGET_IDX]\n",
    "        TARGET_DATE_STR = str(valid_dates[TARGET_IDX])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding date {TARGET_DATE}: {e}\")\n",
    "        # Fallback would happen here if we wanted loopback, but let's just raise/stop for clarity\n",
    "        raise\n",
    "else:\n",
    "    # --- B. Auto Mode (Live or Last Complete) ---\n",
    "    if LIVE_DAY_SERIES is not None:\n",
    "        print(f\"Mode: LIVE Data detected ({LIVE_DAY_DATE})\")\n",
    "        TARGET_PATH_PARTIAL = LIVE_DAY_SERIES\n",
    "        TARGET_DATE_STR = f\"{LIVE_DAY_DATE} (LIVE)\"\n",
    "        IS_LIVE_MODE = True\n",
    "\n",
    "        # In live mode, we simply use the current length of the day\n",
    "        HOURS_TO_TEST = [len(LIVE_DAY_SERIES)]\n",
    "\n",
    "        # Library is full series_matrix (since live day is effectively 'new N+1' day)\n",
    "    else:\n",
    "        print(\"Mode: No incomplete live data. Using LAST complete day.\")\n",
    "        TARGET_IDX = len(series_matrix) - 1\n",
    "\n",
    "        # Remove target from library\n",
    "        library_indices = np.arange(len(series_matrix)) != TARGET_IDX\n",
    "        library_matrix = series_matrix[library_indices]\n",
    "        library_dates = [d for i, d in enumerate(valid_dates) if i != TARGET_IDX]\n",
    "\n",
    "        TARGET_PATH_FULL = series_matrix[TARGET_IDX]\n",
    "        TARGET_DATE_STR = str(valid_dates[TARGET_IDX])\n",
    "\n",
    "\n",
    "# 3. Dynamic Subplot Grid Calculation\n",
    "RANK_STYLES = {\n",
    "    1: {\"color\": \"blue\", \"dash\": \"dot\", \"width\": 2},\n",
    "    2: {\"color\": \"rgba(0, 0, 255, 0.6)\", \"dash\": \"dashdot\", \"width\": 2},\n",
    "    \"default\": {\"color\": \"rgba(0, 0, 255, 0.3)\", \"dash\": \"dash\", \"width\": 1},\n",
    "}\n",
    "\n",
    "NUM_PLOTS = len(HOURS_TO_TEST)\n",
    "COLS = 2\n",
    "ROWS = math.ceil(NUM_PLOTS / COLS)\n",
    "\n",
    "fig_match = make_subplots(\n",
    "    rows=ROWS,\n",
    "    cols=COLS,\n",
    "    subplot_titles=[f\"Input: {h} Hours\" for h in HOURS_TO_TEST],\n",
    "    shared_yaxes=True,\n",
    "    x_title=\"Hour of Day\",\n",
    "    y_title=\"Log Return\",\n",
    ")\n",
    "\n",
    "# 4. Run Matching & Plotting\n",
    "for i, hours_val in enumerate(HOURS_TO_TEST):\n",
    "    # Determine row and col (1-based for Plotly)\n",
    "    r = (i // COLS) + 1\n",
    "    c = (i % COLS) + 1\n",
    "\n",
    "    # a. Slice the input\n",
    "    if IS_LIVE_MODE and TARGET_PATH_PARTIAL is not None:\n",
    "        # Just use what we have (should match hours_val)\n",
    "        partial_input = TARGET_PATH_PARTIAL\n",
    "    elif TARGET_PATH_FULL is not None:\n",
    "        partial_input = TARGET_PATH_FULL[:hours_val]\n",
    "    else:\n",
    "        # Fallback safety\n",
    "        continue\n",
    "\n",
    "    # b. Find nearest neighbors (Top-N)\n",
    "    top_matches = find_nearest_neighbors(partial_input, library_matrix, n_neighbors=NUM_MATCHES)\n",
    "\n",
    "    # c. Plotting\n",
    "\n",
    "    # Trace 1: The Matched Historical Paths (Loop through Top-N)\n",
    "    for match in top_matches:\n",
    "        match_rank = match[\"rank\"]\n",
    "        matched_path_data = match[\"matched_path\"]\n",
    "        matched_idx_lib = match[\"index\"]\n",
    "        matched_date_label = library_dates[matched_idx_lib]\n",
    "\n",
    "        # Vary line style or opacity based on rank if desired\n",
    "        # Rank 1: Blue, Dot\n",
    "        # Rank 2: lighter Blue, DashDot\n",
    "        # Rank 3: cyan?\n",
    "\n",
    "        style = RANK_STYLES.get(match_rank, RANK_STYLES[\"default\"])\n",
    "\n",
    "        show_legend = i == 0  # Only show legend for first subplot to avoid duplicates\n",
    "\n",
    "        fig_match.add_trace(\n",
    "            go.Scatter(\n",
    "                x=time_steps,\n",
    "                y=matched_path_data,\n",
    "                mode=\"lines\",\n",
    "                line={\"color\": style[\"color\"], \"width\": style[\"width\"], \"dash\": style[\"dash\"]},\n",
    "                name=f\"Rank {match_rank}: {matched_date_label}\",\n",
    "                showlegend=show_legend,\n",
    "            ),\n",
    "            row=r,\n",
    "            col=c,\n",
    "        )\n",
    "\n",
    "    # Trace 2: The 'Future' of the Target Path (Ground Truth)\n",
    "    # Only if we have it (Historic Mode)\n",
    "    if not IS_LIVE_MODE and TARGET_PATH_FULL is not None:\n",
    "        fig_match.add_trace(\n",
    "            go.Scatter(\n",
    "                x=time_steps,\n",
    "                y=TARGET_PATH_FULL,\n",
    "                mode=\"lines\",\n",
    "                line={\"color\": \"grey\", \"width\": 1},\n",
    "                name=\"Actual Full Path\",\n",
    "                showlegend=(i == 0),\n",
    "            ),\n",
    "            row=r,\n",
    "            col=c,\n",
    "        )\n",
    "\n",
    "    # Trace 3: The Input Partial Path (What we observed)\n",
    "    fig_match.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps[:hours_val],\n",
    "            y=partial_input,\n",
    "            mode=\"lines+markers\",\n",
    "            line={\"color\": \"black\", \"width\": 3},\n",
    "            marker={\"size\": 6},\n",
    "            name=\"Observed Input\",\n",
    "            showlegend=(i == 0),\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "fig_match.update_layout(\n",
    "    title=f\"Partial Path Matching (Top {NUM_MATCHES}): Simulating Forecasting for {TARGET_DATE_STR}\",\n",
    "    height=max(500, 400 * ROWS),  # Adjust height based on rows\n",
    "    width=1000,\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"x unified\",\n",
    ")\n",
    "\n",
    "fig_match.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
